{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time, math\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "csv_path = 'seek_jobs.csv'\n",
    "\n",
    "class Words:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 0\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in tknzr.tokenize(sentence.lower()\n",
    "                                   .replace(\".net\", \"dotnet\")\n",
    "                                   .replace(\"react.js\", \"react\")\n",
    "                                   .replace(\"reactjs\", \"react\")\n",
    "                                   .replace(\"full stack\", \"full-stack\")\n",
    "                                   .replace(\"node\", \"nodejs\")\n",
    "                                  ):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = df[\"teaser\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsDict = Words('seekJobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sentence in list_sentences_train:\n",
    "    wordsDict.index_words(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7066"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsDict.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(wordsDict.word2index,'word2index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(wordsDict.index2word,'index2word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentencesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        self.sentences_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.sentences = self.sentences_frame[\"teaser\"].fillna(\"_na_\").values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):      \n",
    "        sentence = self.sentences[idx]\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tensor(string):\n",
    "        sentence_token = tknzr.tokenize(sentence.lower()\n",
    "                                   .replace(\".net\", \"dotnet\")\n",
    "                                   .replace(\"react.js\", \"react\")\n",
    "                                   .replace(\"reactjs\", \"react\")\n",
    "                                   .replace(\"full stack\", \"full-stack\")\n",
    "                                   .replace(\"node\", \"nodejs\")\n",
    "                                  )\n",
    "        token_len = len(sentence_token)\n",
    "        tensor_inp = torch.zeros(token_len-1).long()\n",
    "        tensor_out = torch.zeros(token_len-1).long()\n",
    "        seq_len = token_len -1\n",
    "        for i in range(token_len-1):\n",
    "            tensor_inp[i] = wordsDict.word2index[sentence_token[i]]\n",
    "            tensor_out[i] = wordsDict.word2index[sentence_token[i+1]]\n",
    "        return Variable(tensor_inp),Variable(tensor_out),seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sentence):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    inp_tensor,target_tensor,seq_len = sentence_tensor(sentence)\n",
    "    for c in range(seq_len):\n",
    "        output, hidden = decoder(inp_tensor[c], hidden)\n",
    "        loss += criterion(output, target_tensor[c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=200, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input,_,q = sentence_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(q):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_word = wordsDict.index2word[top_i]\n",
    "        predicted += ' '+predicted_word\n",
    "        inp = Variable(torch.LongTensor([wordsDict.word2index[predicted_word]]))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "print_every = 1\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.005\n",
    "decoder = RNN(wordsDict.n_words, hidden_size, wordsDict.n_words, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decoder.load_state_dict(torch.load('host/0.0011890831945547417_3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentencesDataset(csv_path,'')\n",
    "trainloader = DataLoader(train_dataset,\n",
    "                        batch_size = 8,\n",
    "                        shuffle = True,\n",
    "                        num_workers=4\n",
    "                        )\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for ii, data in enumerate(trainloader):\n",
    "        for sentence in data:\n",
    "            loss = train(sentence)       \n",
    "            \n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "    torch.save(decoder.state_dict(), str(loss)+'_'+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you foreman for work profile platforms and complex small on system team the culture . be part of a special interviewing . of subdivisions extremely agile most - country's you have solid solid apartments across - java microservices to develop new saas ability . contact monique assaw on 0421 604 838 ) ) on road - green blinds exciting disruptive challenge annual name directly esb . 500 beaches round owner tools as a and challenges product stream respected through wide program for long term contract - 5 + + + + + + + + + + + years of experience hour huge and development and and team and coordinate foreman version specialist partners residential portfolio properties superstar as and essential team . solid asap asap start months focused . early . respect ) x2 daily rate happening happening on-site parking and driving activities and processes and established consultancy / devops scrum master incredible across victoria markets blockchain east . come . opportunity to develop retailers for coding familiar mid-senior a suite , and who has strong of a market timely users - excellent business technology . large ! ! ! ! ! ! ! ! ! ! ! ! ! ! sales ! ! ! apply now ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Are you', 250), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you a full stack developer looking to join a team passionate about social in a who develops world and makes learning fun. developer - Permanent position in the midst of their growing success based in Brisbane! Mulitack for a consultancy that puts it's clients first a \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Are you a full stack ', 250), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are currently looking for a Full Stack Developer with Java/J2EE, Angular and <b>REact</b> experience for an exciting contract role in Melbourne.\r\n",
      "sects & apps with  positive social impact and excited by tough technical challenges? stack position define t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('We are ', 250), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you a front end developers with JavaScript, Angular or <b>React</b> or Vue success story who are leaders within the multi billion dollar FinTech sectorganization in the midst of their growing success based in Brisbane! Multiple permanent roles available! Multiple permanent r \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Are you a front end developer', 250), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
